{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Nltk.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maddyexplore/py/blob/master/Nltk.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bx-FS6jDuJVV"
      },
      "source": [
        "Nltk (natural Language Tool Kit) is the leading platform for building python programs to work with human language data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uq8admhIuHr0"
      },
      "source": [
        "**Tokenizing**:\n",
        "                  It is a module which further classified into two types namely word trokenize and sent tokenize\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Idfo0QVUsOxW"
      },
      "source": [
        "# NLTK \n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize\n",
        "\n",
        "example_sent = \"\"\"This is a sample sentence. showing off the stop words united states google filtration.\"\"\"\n",
        "print(\"####Tokenizing...\")\n",
        "print(word_tokenize(example_sent))\n",
        "print(sent_tokenize(example_sent))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxqHyS7uvyxG"
      },
      "source": [
        "**Stop_Words**:\n",
        "                      These are the most commonly used words such as -- is, the, a, an, in, etc...using this we can filter out the useless data.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEyfFdFstfKt"
      },
      "source": [
        "from nltk.corpus import stopwords, wordnet                                                                                                                                                                                                               # stop words\n",
        "\n",
        "example_sent = \"\"\"This is a sample sentence. showing off the stop words united states google filtration.\"\"\"\n",
        "print(\"#####stop_words\")\n",
        "print(stopwords.words('english'))\n",
        "print()\n",
        "print(\"####words without stop_words\")\n",
        "filtered_words = [words for words in word_tokenize(example_sent) if not words in stopwords.words('english') and words.isalpha()]\n",
        "print(filtered_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dAxniX_xGcC"
      },
      "source": [
        "**Stemming**:\n",
        "                It is a technique used to extract the base form of the words by removing affixes from them"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3DLyHywzjUO"
      },
      "source": [
        "from nltk.stem import PorterStemmer, LancasterStemmer, RegexpStemmer, wordnet\n",
        "\n",
        "words = [\"code\",\"coding\",\"coders\",\"codery\"]\n",
        "ps = PorterStemmer() # stemming least ag\n",
        "ls = LancasterStemmer() # very agrressive\n",
        "rs = RegexpStemmer(\"ing\")\n",
        "print(\"####Porter Stemming...\")\n",
        "for word in words:\n",
        "    print(ps.stem(word))\n",
        "\n",
        "print()\n",
        "print(\"####Lancaster Stemming...\")\n",
        "for word in words:\n",
        "    print(ls.stem(word))\n",
        "\n",
        "print()\n",
        "print(\"#### Regexp Stemming...\")\n",
        "for word in words:\n",
        "    print(rs.stem(word))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0D8qXsD0Ict"
      },
      "source": [
        "**Lemmantizing**:\n",
        "                              Lemmatization is similar to stemming but it brings context to the words.\n",
        "                              It refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olJ0N9MA0tPP"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "lm = WordNetLemmatizer()\n",
        "print()\n",
        "print(\"#### Lemmantizing\")\n",
        "print(lm.lemmatize(\"better\",\"a\"))\n",
        "print(lm.lemmatize(\"better\"))\n",
        "print(lm.lemmatize(\"corpora\"))\n",
        "print(lm.lemmatize(\"mice\"))\n",
        "print(lm.lemmatize(\"babies\"))\n",
        "print(lm.lemmatize(\"is\",\"v\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NJo3Jl2W0-Ua"
      },
      "source": [
        "**POS_TAG**:\n",
        "                POS Tagging in NLTK is a process to mark up the words in text format for a particular part of a speech based on its definition and context.\n",
        "                Example:\n",
        "                NN - noun ,\n",
        "                VB - verb,\n",
        "                RB - Adverb,\n",
        "                PRP - pronoun,\n",
        "                JJ - adjective..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ciMLAqh91bfT"
      },
      "source": [
        "print(\"#### POS Tagging\")    \n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "pos_words = [words for words in word_tokenize(example_sent) if not words in stopwords.words('english') and words.isalpha()] # isalpha to remove punctuation\n",
        "print(nltk.pos_tag(pos_words))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Z6_bBIU49C2"
      },
      "source": [
        "**Chunking & Chinking**:\n",
        "                                      process of grouping similar words together based on the nature of the word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNwo4ppj5Jfq"
      },
      "source": [
        "postag = nltk.pos_tag(pos_words)\n",
        "\n",
        "print(\"#### chunking of words...\")\n",
        "\n",
        "# grammer \"NP: {}\"                                                                                                                                                         #Chunking\n",
        "grammer = \"\"\"Chunk: {<.*>+}\n",
        "\t\t\t\t    }<DT|VBG>{ \"\"\"                                                                                                                                                  #Chinking \n",
        "\n",
        "chunk = nltk.RegexpParser(grammer)\n",
        "res = chunk.parse(postag)\n",
        "\n",
        "res = nltk.ne_chunk(nltk.pos_tag(word_tokenize(example_sent)))\n",
        "print(res)\n",
        "#res.draw()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "triJ-u7y6BMY"
      },
      "source": [
        "  **Wordnet**:\n",
        "                It is an NLTK corpus reader, a lexical database for English. It can be used to find the meaning of words, synonym or antonym."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "muTgOkVp6Nc4"
      },
      "source": [
        "from nltk.corpus import wordnet as wordnet\n",
        "syns = wordnet.synsets(\"good\")\n",
        "s,a = [],[]\n",
        "print(syns[0].definition())\n",
        "for i in syns:                                                                              # lemma => good.n.01--> n --> noun (pos) \n",
        "    for l in i.lemmas():\n",
        "        s.append(l.name())\n",
        "        if l.antonyms():\n",
        "            a.append(l.antonyms()[0].name())\n",
        "\n",
        "print(set(s))\n",
        "print(set(a))\n",
        "\n",
        "# WuPalmer\n",
        "\n",
        "w1 = wordnet.synset(\"ship.n.01\")\n",
        "w2 = wordnet.synset(\"boat.n.01\")\n",
        "\n",
        "print(w1.wup_similarity(w2))\n",
        "\n",
        "w1 = wordnet.synset(\"mother.n.01\")\n",
        "w2 = wordnet.synset(\"child.n.01\")\n",
        "\n",
        "print(w1.wup_similarity(w2))\n",
        "\n",
        "w1 = wordnet.synset(\"mother.n.01\")\n",
        "w2 = wordnet.synset(\"father.n.01\")\n",
        "\n",
        "print(w1.wup_similarity(w2))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}